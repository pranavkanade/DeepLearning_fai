{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"s03.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"ZIbhHBDl2NhP","colab_type":"text"},"cell_type":"markdown","source":["# connecting to the drive"]},{"metadata":{"id":"3ijbl2rk14Gr","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":4}],"base_uri":"https://localhost:8080/","height":108},"outputId":"62173c22-8c8f-44ab-ebc2-47b46e8ce666","executionInfo":{"status":"ok","timestamp":1517681289778,"user_tz":-330,"elapsed":20364,"user":{"displayName":"pranav kanade","photoUrl":"//lh3.googleusercontent.com/-2xb38FINcVg/AAAAAAAAAAI/AAAAAAAAAK4/bX4wnr-4dt0/s50-c-k-no/photo.jpg","userId":"106032156672012481149"}}},"cell_type":"code","source":["!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n","!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n","!apt-get update -qq 2>&1 > /dev/null\n","!apt-get -y install -qq google-drive-ocamlfuse fuse\n","from google.colab import auth\n","auth.authenticate_user()\n","from oauth2client.client import GoogleCredentials\n","creds = GoogleCredentials.get_application_default()\n","import getpass\n","!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n","vcode = getpass.getpass()\n","!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n","!mkdir -p drive\n","!google-drive-ocamlfuse drive"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\r\n","··········\n","Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n","Please enter the verification code: Access token retrieved correctly.\n"],"name":"stdout"}]},{"metadata":{"id":"ianqjbhA2RjX","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":34},"outputId":"d47adb1e-4aba-43ce-a728-8f1df00c62b8","executionInfo":{"status":"ok","timestamp":1517698631490,"user_tz":-330,"elapsed":880,"user":{"displayName":"pranav kanade","photoUrl":"//lh3.googleusercontent.com/-2xb38FINcVg/AAAAAAAAAAI/AAAAAAAAAK4/bX4wnr-4dt0/s50-c-k-no/photo.jpg","userId":"106032156672012481149"}}},"cell_type":"code","source":["!ls"],"execution_count":1,"outputs":[{"output_type":"stream","text":["~  data  datalab  drive  neg.txt  nltk_data  pos.txt  so3_lexicons.pickle\r\n"],"name":"stdout"}]},{"metadata":{"id":"UM8gPxGJ2tyc","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["!cp drive/DeepLearning/*.txt ./    # copy the data from the drive to the vm "],"execution_count":0,"outputs":[]},{"metadata":{"id":"l6oCw8LY27-C","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":34},"outputId":"6974d4b3-94d9-4ea1-86ed-411ffca056a9","executionInfo":{"status":"ok","timestamp":1517699767362,"user_tz":-330,"elapsed":2449,"user":{"displayName":"pranav kanade","photoUrl":"//lh3.googleusercontent.com/-2xb38FINcVg/AAAAAAAAAAI/AAAAAAAAAK4/bX4wnr-4dt0/s50-c-k-no/photo.jpg","userId":"106032156672012481149"}}},"cell_type":"code","source":["!ls"],"execution_count":11,"outputs":[{"output_type":"stream","text":["~  data  datalab  drive  neg.txt  nltk_data  pos.txt  so3_lexicons.pickle\r\n"],"name":"stdout"}]},{"metadata":{"id":"pae2VcHn2-ay","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["!pip freeze"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-aR4avv33PHC","colab_type":"text"},"cell_type":"markdown","source":["# sentiment"]},{"metadata":{"id":"siN9TyS6LTFm","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import nltk"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0Apawx_9kkw0","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","import numpy as np\n","from collections import Counter\n","import random\n","import pickle"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bo9oG4fZl0eC","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["lemmatizer = WordNetLemmatizer()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"A6oRTezhmVld","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["hm_lines = 10000000"],"execution_count":0,"outputs":[]},{"metadata":{"id":"15nbI81Jmbob","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# need to create the lexicons\n","\n","def create_lexicons(pos, neg):\n","  lexicons = []\n","  for fi in [pos, neg]:\n","    with open(fi, 'r') as f:\n","      contents = f.readlines()\n","      for line in contents[:hm_lines]:\n","        all_words = word_tokenize(line.lower())\n","        lexicons += list(all_words)\n","        \n","  lexicons = [lemmatizer.lemmatize(l) for l in lexicons]\n","  w_counts = Counter(lexicons)\n","  \n","  lexcon = []\n","  \n","  for w in w_counts:\n","    if 1000 > w_counts[w] > 50:\n","      lexcon.append(w)\n","  print('number of lexicons found : ', len(lexcon))\n","  return(lexcon)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0Pb8Bgh5phxM","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# This function is responsible for featching the data from the source and make samples\n","\n","def sample_handling(sample, lexicon, classification):\n","  featureset = []\n","  with open(sample, 'r') as f:\n","    contents = f.readlines()\n","    for l in contents[:hm_lines]:\n","      current_words = word_tokenize(l.lower())\n","      current_words = [lemmatizer.lemmatize(w) for w in current_words]\n","      features = np.zeros(len(lexicon))\n","      for word in current_words:\n","        if word.lower() in lexicon:\n","          word_indx = lexicon.index(word.lower())\n","          features[word_indx] = 1\n","          \n","      features = list(features)\n","      featureset.append([features, classification])\n","      \n","      \n","  return featureset"],"execution_count":0,"outputs":[]},{"metadata":{"id":"F_zi_GRntDcL","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# creating the set of data to train and test out model that we'll be creating \n","def create_feature_sets_and_labels(pos, neg, test_size=0.1):\n","  lexicon = create_lexicons(pos, neg)\n","  features = []\n","  features += sample_handling('pos.txt', lexicon, [1, 0])\n","  features += sample_handling('neg.txt', lexicon, [0, 1])\n","  random.shuffle(features)\n","  \n","  features = np.array(features)\n","  \n","  testing_size = int(test_size*len(features))\n","  \n","  train_x = list(features[:,0][:-testing_size])\n","  train_y = list(features[:,1][:-testing_size])\n","  \n","  test_x = list(features[:,0][-testing_size:])\n","  test_y = list(features[:,1][-testing_size:])\n","  \n","  return train_x, train_y, test_x, test_y"],"execution_count":0,"outputs":[]},{"metadata":{"id":"mR0PZIWkv_hp","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":34},"outputId":"055a00c9-c3bb-4ec9-d572-7411034e5c99","executionInfo":{"status":"ok","timestamp":1517699814180,"user_tz":-330,"elapsed":25452,"user":{"displayName":"pranav kanade","photoUrl":"//lh3.googleusercontent.com/-2xb38FINcVg/AAAAAAAAAAI/AAAAAAAAAK4/bX4wnr-4dt0/s50-c-k-no/photo.jpg","userId":"106032156672012481149"}}},"cell_type":"code","source":["# this is the main cell where we call function to actually create the sample data\n","# and store the generated data to pickled file\n","\n","train_x, train_y, test_x, test_y = create_feature_sets_and_labels('pos.txt', 'neg.txt')\n","with open('so3_lexicons.pickle', 'wb') as pk:\n","  pickle.dump([train_x, train_y, test_x, test_y], pk)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["number of lexicons found :  423\n"],"name":"stdout"}]},{"metadata":{"id":"ieu3DRyVxdpO","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":34},"outputId":"330c4633-d9e3-4f12-deca-69bd860907b1","executionInfo":{"status":"ok","timestamp":1517696820442,"user_tz":-330,"elapsed":1076,"user":{"displayName":"pranav kanade","photoUrl":"//lh3.googleusercontent.com/-2xb38FINcVg/AAAAAAAAAAI/AAAAAAAAAK4/bX4wnr-4dt0/s50-c-k-no/photo.jpg","userId":"106032156672012481149"}}},"cell_type":"code","source":["!ls"],"execution_count":31,"outputs":[{"output_type":"stream","text":["~  data  datalab  drive  neg.txt  nltk_data  pos.txt  so3_lexicons.pickle\r\n"],"name":"stdout"}]},{"metadata":{"id":"2pa_1Xmfx6BZ","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":353},"outputId":"8d9f108c-1440-4deb-f174-df62a8cca4d0","executionInfo":{"status":"ok","timestamp":1517696835613,"user_tz":-330,"elapsed":1516,"user":{"displayName":"pranav kanade","photoUrl":"//lh3.googleusercontent.com/-2xb38FINcVg/AAAAAAAAAAI/AAAAAAAAAK4/bX4wnr-4dt0/s50-c-k-no/photo.jpg","userId":"106032156672012481149"}}},"cell_type":"code","source":["!ls -la    # look at the last entry in the out put `so3_lexicons.pickle`"],"execution_count":32,"outputs":[{"output_type":"stream","text":["total 137948\r\n","drwxr-xr-x  3 root root      4096 Feb  3 16:19 ~\r\n","drwxr-xr-x  1 root root      4096 Feb  3 22:26 .\r\n","drwxr-xr-x  1 root root      4096 Feb  3 15:41 ..\r\n","drwx------  4 root root      4096 Feb  3 15:42 .cache\r\n","drwxr-xr-x  3 root root      4096 Feb  3 15:42 .config\r\n","drwxr-xr-x  2 root root      4096 Feb  3 16:21 data\r\n","drwxr-xr-x  1 root root      4096 Feb  3 18:07 datalab\r\n","drwxr-xr-x  2 root root      4096 Feb  3 18:08 drive\r\n","drwxr-xr-x  4 root root      4096 Feb  3 15:41 .forever\r\n","drwx------  3 root root      4096 Feb  3 18:07 .gdfuse\r\n","drwxr-xr-x  5 root root      4096 Feb  3 15:42 .ipython\r\n","drwxr-xr-x  2 root root      4096 Feb  3 15:46 .keras\r\n","drwx------  3 root root      4096 Feb  3 15:42 .local\r\n","-rw-r--r--  1 root root    612459 Feb  3 19:29 neg.txt\r\n","drwxr-xr-x 12 root root      4096 Feb  3 21:29 nltk_data\r\n","drwx------  3 root root      4096 Feb  3 16:02 .nv\r\n","-rw-r--r--  1 root root    626349 Feb  3 19:29 pos.txt\r\n","-rw-------  1 root root      1024 Feb  3 15:41 .rnd\r\n","-rw-r--r--  1 root root 139948854 Feb  3 22:26 so3_lexicons.pickle\r\n"],"name":"stdout"}]},{"metadata":{"id":"O_Rfjny6zix8","colab_type":"text"},"cell_type":"markdown","source":["# Deep learining on the above sample generated"]},{"metadata":{"id":"8tt86mkHx9nZ","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":34},"outputId":"1a56c0c9-15a5-4eb3-d461-9661abf43022","executionInfo":{"status":"ok","timestamp":1517699833108,"user_tz":-330,"elapsed":10141,"user":{"displayName":"pranav kanade","photoUrl":"//lh3.googleusercontent.com/-2xb38FINcVg/AAAAAAAAAAI/AAAAAAAAAK4/bX4wnr-4dt0/s50-c-k-no/photo.jpg","userId":"106032156672012481149"}}},"cell_type":"code","source":["import tensorflow as tf\n","\n","train_x, train_y, test_x, test_y = create_feature_sets_and_labels('pos.txt', 'neg.txt')\n","\n","n_nodes_hl1 = 500\n","n_nodes_hl2 = 500\n","n_nodes_hl3 = 500\n","\n","# number of classes : only 2 (positive and negetive sentiment)\n","n_classes = 2\n","\n","# batch size is the number of images in each iteration that the network will be able to see \n","batch_size = 100\n","\n","# shape of our input array which is size of lexicon or length of single input train_x[0]\n","input_shape = len(train_x[0])\n","\n","# we'll be streaching out whole matrix to work as a single dimensional array\n","\n","# lets define the variables :\n","# input : x and output : y\n","x = tf.placeholder('float', [None, input_shape])\n","y = tf.placeholder('float', [None, 2])"],"execution_count":20,"outputs":[{"output_type":"stream","text":["number of lexicons found :  423\n"],"name":"stdout"}]},{"metadata":{"id":"vYaOGLnx39LA","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def neural_network_model(input_data):\n","  hidden_layer1 = {'weights': tf.Variable(tf.random_normal([input_shape, n_nodes_hl1])), \n","                   'biases': tf.Variable(tf.random_normal([n_nodes_hl1]))}\n","  \n","  \n","  hidden_layer2 = {'weights': tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])), \n","                   'biases': tf.Variable(tf.random_normal([n_nodes_hl2]))}\n","  \n","  \n","  hidden_layer3 = {'weights': tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])), \n","                   'biases': tf.Variable(tf.random_normal([n_nodes_hl3]))}\n","  \n","  \n","  output_layer = {'weights': tf.Variable(tf.random_normal([n_nodes_hl3, n_classes])), \n","                   'biases': tf.Variable(tf.random_normal([n_classes]))}\n","  \n","  # model for each layer\n","  # input_data * weights + biases = result for next layer\n","  \n","  l1 = tf.add(tf.matmul(input_data, hidden_layer1['weights']), hidden_layer1['biases'])\n","  l1 = tf.nn.relu(l1)\n","  \n","  l2 = tf.add(tf.matmul(l1, hidden_layer2['weights']), hidden_layer2['biases'])\n","  l2 = tf.nn.relu(l2)\n","\n","  l3 = tf.add(tf.matmul(l2, hidden_layer3['weights']), hidden_layer3['biases'])\n","  l3 = tf.nn.relu(l3)\n","  \n","  output = tf.add(tf.matmul(l3, output_layer['weights']), output_layer['biases'])\n","  \n","  return output"],"execution_count":0,"outputs":[]},{"metadata":{"id":"siRkwPqd7PWt","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["\n","# this is the training function that will be very handy we'll use the output of the model here\n","def train_neural_network(x):\n","  prediction = neural_network_model(x)\n","  \n","  # we need to minimize the cost as it denotes the difference between the prediction that we are making\n","  # and the actual lable\n","  cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=prediction))\n","  \n","  # need to optimize this\n","  optimizer = tf.train.AdamOptimizer().minimize(cost)\n","  \n","  n_epoch = 20\n","  \n","  with tf.Session() as sess:\n","    sess.run(tf.initialize_all_variables())\n","    \n","    for epoch in range(n_epoch):\n","      e_loss = 0\n","      \n","      '''\n","      # these two lines were very specific to mnist data\n","      for _ in range(mnist.train.num_examples//batch_size):\n","        e_x, e_y = mnist.train.next_batch(batch_size)\n","        \n","      '''\n","      \n","      idx = 0\n","      while idx < len(train_x):\n","        init = idx\n","        end = idx + batch_size\n","        idx = end\n","        \n","        batch_x = np.array(train_x[init:end])\n","        batch_y = np.array(train_y[init:end])\n","        \n","        _, c = sess.run([optimizer, cost], feed_dict={x: batch_x, y: batch_y})\n","        e_loss += c\n","        \n","      print(\"Epoch_loss : \",e_loss)\n","      \n","      \n","    correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n","    accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n","    \n","    print('Accuracy : ', accuracy.eval({x: test_x, y: test_y}))\n","\n","train_neural_network(x)\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"IbornmIH8v7c","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":34},"outputId":"35e652ad-4ef2-44a3-a165-516a4f7b0e8b","executionInfo":{"status":"ok","timestamp":1517700172987,"user_tz":-330,"elapsed":797,"user":{"displayName":"pranav kanade","photoUrl":"//lh3.googleusercontent.com/-2xb38FINcVg/AAAAAAAAAAI/AAAAAAAAAK4/bX4wnr-4dt0/s50-c-k-no/photo.jpg","userId":"106032156672012481149"}}},"cell_type":"code","source":["!ls"],"execution_count":1,"outputs":[{"output_type":"stream","text":["~  data  datalab  drive  neg.txt  nltk_data  pos.txt  so3_lexicons.pickle\r\n"],"name":"stdout"}]},{"metadata":{"id":"cd17xe_B-sk2","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["!cp so3_lexicons.pickle drive/DeepLearning/"],"execution_count":0,"outputs":[]},{"metadata":{"id":"awbBmiu9_DIN","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}